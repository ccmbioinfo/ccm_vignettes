{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c041c787-75e2-4037-8118-d3de5b7b4479",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967e739-189e-458f-9470-c5dcac98fe05",
   "metadata": {},
   "source": [
    "Hugginface has many packages most of the model related functionalities are in the transformes package, other packages like datasets, and tokenizers are there to support training/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de29de51-33d8-4638-9a8f-29719d74a908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e54a6-61a2-48c6-9490-6f738540a536",
   "metadata": {},
   "source": [
    "pipeline is the generic wrapper for inference, you can specify a lot of nlp and computer vision tasks and associated models, the model is downloaded automatically in your ~/.cache you can also specify where the model can be downloed. The wrapper olso downloads the appropriate tokenizer that goes with the model. When a model is uploaded to huggingface along with all the necessary files to run it. If a model is trained using a specific framework (torch, tensorflow) it is uploaded in that framework, there is no built in cross-compatbility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc483a3f-0cb5-47b5-ae01-2dfee4643e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715ae83fb7314f82b15fffd81fc8dc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae45849df7644c1b8b0a1653ce277178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7ffb3f211440acbdb63fa19a5c8fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b333853d090544b4a3a5e38534b97db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "classifier=pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80de4348-8cd7-430a-84ce-a093d7b25adb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998750686645508}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"this is an awesome package\"\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457fc062-e969-481a-8cf0-4da38163c64b",
   "metadata": {},
   "source": [
    "the label depends on how the training is done, it does not have to be binary classifcation. \n",
    "\n",
    "There are many tasks and associated models that go with it. Some are bigger than others and it is generally good idea to have a GPU available when we are training/fine-tuning or when we have 1000s of samples to run inference on. If the model is bigger than the avaiable VRAM then you will most likely get an Out of memory error. There is not much you can do about it. Huggingface supports distributed training/inference, that is you can split the model into multiple gpus (assuming that they can talk to one another). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1052c5-1217-4d83-ba1a-3ce4a0a3185f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9459587b74584599a2bf4bddd82210bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab5a7b5780f4d8bb72381a33ac7ddd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eeee9ecff784b41b2b05246bad9929a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea6cbd97dbe4b0cb89bdd051ff00793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780a78d71243448fbc525905b4847656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarizer = pipeline('summarization', model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "text=\"\"\"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an \n",
    "encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. \n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \n",
    "Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly \n",
    "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, \n",
    "including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art \n",
    "BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n",
    "We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c165f0-7626-41cb-a180-5a44545f706e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration . The best performing models also connect the encoder and decoder through an attention mechanism . We propose a new simple network architecture, the Transformer, based solely on attention mechanisms .'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfad081f-e546-4cd7-9ffb-4a812d98428b",
   "metadata": {},
   "source": [
    "0 shot classification is not really a built in classification but rather comparison of embeddings of the labels with the embeddings of the text and we can find the closest one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b020718-cf11-496f-8c28-84eb3293a820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20a98fd5e4a4036b5a33d93a9f0233b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb9c32d7e8c46628ca7032a7ff91cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/369M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f3c0b4aba54a5c9299d7d0179679a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7368cf012f7486586a4115f5fc33e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c725b5e9ece4415b7558c9e775e94a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/8.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555e5cf78ddb453297fdf868c42e4a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8128bc4b01504170816db5ebea29a315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'hospital for sick children is a major research centre',\n",
       " 'labels': ['science', 'medicine', 'economy', 'politics', 'entertainment'],\n",
       " 'scores': [0.5817810297012329,\n",
       "  0.4062599837779999,\n",
       "  0.005329915322363377,\n",
       "  0.003325793659314513,\n",
       "  0.003303299890831113]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\")\n",
    "text=\"hospital for sick children is a major research centre\"\n",
    "candidate_labels = [\"politics\", \"economy\", \"entertainment\", \"science\", \"medicine\"]\n",
    "classifier(text, candidate_labels, multi_label=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6809fb4-9313-47a9-892c-c84dc0b4f970",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'hospital for sick children is a major research centre',\n",
       " 'labels': ['science', 'medicine', 'economy', 'politics', 'entertainment'],\n",
       " 'scores': [0.9735254645347595,\n",
       "  0.9540853500366211,\n",
       "  0.00026499555679038167,\n",
       "  0.0001278855197597295,\n",
       "  8.470423199469224e-05]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multi label\n",
    "classifier(text, candidate_labels, multi_label=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa549d-bdc7-4635-ab35-03cfc87aa5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers #if not installed ! means run the shell command "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "814e1842-d827-4bbe-bfe5-c6a4cdc3a627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sent1=\"I like eating hamburgers\"\n",
    "sent2=\"I like eating burgers and fries\"\n",
    "sent3=\"I do research on DNA sequences\"\n",
    "\n",
    "em1=model.encode(sent1)\n",
    "em2=model.encode(sent2)\n",
    "em3=model.encode(sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "093247ff-33d4-4560-bb56-7756de79e86a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((384,), (384,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em1.shape, em2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d1b774-5b1b-4501-a9b5-0e38bfb95078",
   "metadata": {},
   "source": [
    "Sentence transformers package contains a lot of models and utility functions for semantic similarity and other comparison methods. The cosinde similarity is literally the angle between the 2 embedding vectors, cosine of 0 is 1, which means if the angle between the vector is 0 (they are on top of one another) then they are identical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42b634a3-f7cb-4a94-99f9-c83738d33a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8372]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.cos_sim(em1, em2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e901eb3a-7282-49f6-a9c3-07b686914378",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1328]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.cos_sim(em1, em3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f327d5-c50e-427f-beaf-749617199073",
   "metadata": {},
   "source": [
    "It is possible to do the same with models that are not in sentences transformers with a little bit more code, we can use the 'features extraction' pipeline to get the embeddings of the text we are interested in and compare it to another set of embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86d10323-3ef3-483b-a084-96ff81649012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# it is also possible to load model and a tokenizer separately and combine them in a pipeline, this is useful if you have a bunch of models that are different sizes, trained on differnt data but all use the same tokenizer\n",
    "model=AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# model max length is pre defined by it's attention mechanism and the size of the embeddings that it can take, the reason for that is due to the size of the transformer layers, they are fixed size (there are other methods that can strech this a little bit). \n",
    "tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", max_length=512, truncate=True, padding=\"max_length\")\n",
    "\n",
    "# build the pipeline piece by piece\n",
    "# use a pytorch model and return an pytorch tensor\n",
    "extractor=pipeline(\"feature-extraction\", framework=\"pt\", model=model, tokenizer=tokenizer, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2219d6a-0e42-4079-8f5e-36ae69866f93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_sentence=\"This is a short sentence\"\n",
    "long_sentence=\"This sentence is considerable longer than the first one, while being still shorter than the model max length that is 512 words, it is also similar to the first sentence\"\n",
    "other_long_sentence=\"In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since. And then I met Gatsby!\"\n",
    "\n",
    "short_tokenized=tokenizer(short_sentence)\n",
    "long_tokenized=tokenizer(long_sentence)\n",
    "other_long_tokenized=tokenizer(other_long_sentence)\n",
    "\n",
    "short_tokenized\n",
    "short_tokenized[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59d01199-b305-426c-a4a0-db324f888289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2023, 2003, 1037, 2460, 6251, 102]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_tokenized[\"input_ids\"] # the integers corresponding to each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1f8fe30-4de8-4184-9a63-3d0790310d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] this is a short sentence [SEP]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(short_tokenized[\"input_ids\"]) # the model is uncased so everything is always lower case, this means the words Alper and alper are the same, there are cased models where there is a difference, you can pick whiever suits your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cef14f84-f709-42d1-804e-489492cbe48f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 768]), torch.Size([34, 768]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_features=extractor(short_sentence)\n",
    "long_features=extractor(long_sentence)\n",
    "\n",
    "short_features[0].shape, long_features[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c2e954-f010-483c-9401-bee68116b426",
   "metadata": {},
   "source": [
    "We cannot compare tensors/matrices of different sizes, we need to somewhow need to convert them to the same shape, one way to do that is to take the mean across one dimension and end up with the same lenth vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "634e8dac-307b-44fb-b738-42ac5d0e5100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "short_mean=short_features[0].mean(axis=0)\n",
    "long_mean=long_features[0].mean(axis=0)\n",
    "\n",
    "from torch.nn import CosineSimilarity\n",
    "from torch.linalg import norm\n",
    "\n",
    "cos_sim=CosineSimilarity(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f817792-d522-4147-9477-74d20b944354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7598)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(short_mean, long_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cbbf69f-aca1-40a8-8d2b-255e82e82988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8372)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(torch.from_numpy(em1), torch.from_numpy(em2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccf8933c-9d93-4568-8d72-6b797b722697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1328)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(torch.from_numpy(em1), torch.from_numpy(em3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77cc289-bc43-490a-bd5d-cbb8b5953a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
