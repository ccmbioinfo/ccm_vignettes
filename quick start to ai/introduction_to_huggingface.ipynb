{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction to Huggingface\n",
    "\n",
    "This notebook illustrates how to perfrom some basic actions using the huggingface, transformers ecosystem. We will not be focusing on biology but how to accomplish common AI tasks. While we have chose small models and you should be able to run them on a CPU having access to a GPU is highly reccomended. Additionally some of these commands will download datasets and models to the environment you are running the code. Make sure you have the write permissions and enough storage space."
   ],
   "id": "285d306f90516118"
  },
  {
   "metadata": {
    "tags": []
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1,
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "id": "2cfe65f989d00a53"
  },
  {
   "cell_type": "markdown",
   "id": "c967e739-189e-458f-9470-c5dcac98fe05",
   "metadata": {},
   "source": [
    "Hugginface has many packages most of the model related functionalities are in the transformes package, other packages like datasets, and tokenizers are there to support training/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de29de51-33d8-4638-9a8f-29719d74a908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e54a6-61a2-48c6-9490-6f738540a536",
   "metadata": {},
   "source": [
    "pipeline is the generic wrapper for inference, you can specify a lot of nlp and computer vision tasks and associated models, the model is downloaded automatically in your ~/.cache you can also specify where the model can be downloed. The wrapper olso downloads the appropriate tokenizer that goes with the model. When a model is uploaded to huggingface along with all the necessary files to run it. If a model is trained using a specific framework (torch, tensorflow) it is uploaded in that framework, there is no built in cross-compatbility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc483a3f-0cb5-47b5-ae01-2dfee4643e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715ae83fb7314f82b15fffd81fc8dc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae45849df7644c1b8b0a1653ce277178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7ffb3f211440acbdb63fa19a5c8fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b333853d090544b4a3a5e38534b97db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "classifier=pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80de4348-8cd7-430a-84ce-a093d7b25adb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998750686645508}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"this is an awesome package\"\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457fc062-e969-481a-8cf0-4da38163c64b",
   "metadata": {},
   "source": [
    "the label depends on how the training is done, it does not have to be binary classifcation. \n",
    "\n",
    "There are many tasks and associated models that go with it. Some are bigger than others and it is generally good idea to have a GPU available when we are training/fine-tuning or when we have 1000s of samples to run inference on. If the model is bigger than the avaiable VRAM then you will most likely get an Out of memory error. There is not much you can do about it. Huggingface supports distributed training/inference, that is you can split the model into multiple gpus (assuming that they can talk to one another). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1052c5-1217-4d83-ba1a-3ce4a0a3185f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9459587b74584599a2bf4bddd82210bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab5a7b5780f4d8bb72381a33ac7ddd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eeee9ecff784b41b2b05246bad9929a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea6cbd97dbe4b0cb89bdd051ff00793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780a78d71243448fbc525905b4847656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarizer = pipeline('summarization', model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "text=\"\"\"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an \n",
    "encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. \n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \n",
    "Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly \n",
    "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, \n",
    "including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art \n",
    "BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n",
    "We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c165f0-7626-41cb-a180-5a44545f706e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration . The best performing models also connect the encoder and decoder through an attention mechanism . We propose a new simple network architecture, the Transformer, based solely on attention mechanisms .'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfad081f-e546-4cd7-9ffb-4a812d98428b",
   "metadata": {},
   "source": "0 shot classification is not really a built in classification but rather comparison of embeddings of the labels with the embeddings of the text and we can find the closest one. Here embeddings refer to the models \"understanding\" of the input. These are represented as a bunch of number as a `pytorch.Tensor`. Once we calculate these embeddings for our query *and* labels we can calculate the  most similar one."
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b020718-cf11-496f-8c28-84eb3293a820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20a98fd5e4a4036b5a33d93a9f0233b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb9c32d7e8c46628ca7032a7ff91cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/369M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f3c0b4aba54a5c9299d7d0179679a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7368cf012f7486586a4115f5fc33e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c725b5e9ece4415b7558c9e775e94a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/8.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555e5cf78ddb453297fdf868c42e4a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8128bc4b01504170816db5ebea29a315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'hospital for sick children is a major research centre',\n",
       " 'labels': ['science', 'medicine', 'economy', 'politics', 'entertainment'],\n",
       " 'scores': [0.5817810297012329,\n",
       "  0.4062599837779999,\n",
       "  0.005329915322363377,\n",
       "  0.003325793659314513,\n",
       "  0.003303299890831113]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\")\n",
    "text=\"hospital for sick children is a major research centre\"\n",
    "candidate_labels = [\"politics\", \"economy\", \"entertainment\", \"science\", \"medicine\"]\n",
    "classifier(text, candidate_labels, multi_label=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6809fb4-9313-47a9-892c-c84dc0b4f970",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'hospital for sick children is a major research centre',\n",
       " 'labels': ['science', 'medicine', 'economy', 'politics', 'entertainment'],\n",
       " 'scores': [0.9735254645347595,\n",
       "  0.9540853500366211,\n",
       "  0.00026499555679038167,\n",
       "  0.0001278855197597295,\n",
       "  8.470423199469224e-05]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multi label\n",
    "classifier(text, candidate_labels, multi_label=True)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since the writing of this article the sentence transformers library has been merged into transformers, the functionality of the package remains mostly unchanged but it may in the future. Sentense transformers package focuses on generating the embeddings we mentioned above. They can be generated in bulk for diverse texts. You can treat these embeddigns like any other feature of your dataset and can perfrom operations like classification, regresssion or clustering. Each element of the tensor is not a meaninigful feature but the whole vector is the models' understanding of where the piece of text sits withing the available space.",
   "id": "29c2ffc626d58b13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa549d-bdc7-4635-ab35-03cfc87aa5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "!pip install sentence_transformers #if not installed ! means run the shell command"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "814e1842-d827-4bbe-bfe5-c6a4cdc3a627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sent1=\"I like eating hamburgers\"\n",
    "sent2=\"I like eating burgers and fries\"\n",
    "sent3=\"I do research on DNA sequences\"\n",
    "\n",
    "em1=model.encode(sent1)\n",
    "em2=model.encode(sent2)\n",
    "em3=model.encode(sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "093247ff-33d4-4560-bb56-7756de79e86a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((384,), (384,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em1.shape, em2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d1b774-5b1b-4501-a9b5-0e38bfb95078",
   "metadata": {},
   "source": [
    "Sentence transformers package contains a lot of models and utility functions for semantic similarity and other comparison methods. The cosinde similarity is literally the angle between the 2 embedding vectors, cosine of 0 is 1, which means if the angle between the vector is 0 (they are on top of one another) then they are identical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42b634a3-f7cb-4a94-99f9-c83738d33a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8372]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.cos_sim(em1, em2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e901eb3a-7282-49f6-a9c3-07b686914378",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1328]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.cos_sim(em1, em3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f327d5-c50e-427f-beaf-749617199073",
   "metadata": {},
   "source": [
    "It is possible to do the same with models that are not in sentences transformers with a little bit more code, we can use the 'features extraction' pipeline to get the embeddings of the text we are interested in and compare it to another set of embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86d10323-3ef3-483b-a084-96ff81649012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# it is also possible to load model and a tokenizer separately and combine them in a pipeline, this is useful if you have a bunch of models that are different sizes, trained on differnt data but all use the same tokenizer\n",
    "model=AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# model max length is pre defined by it's attention mechanism and the size of the embeddings that it can take, the reason for that is due to the size of the transformer layers, they are fixed size (there are other methods that can strech this a little bit). \n",
    "tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", max_length=512, truncate=True, padding=\"max_length\")\n",
    "\n",
    "# build the pipeline piece by piece\n",
    "# use a pytorch model and return an pytorch tensor\n",
    "extractor=pipeline(\"feature-extraction\", framework=\"pt\", model=model, tokenizer=tokenizer, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2219d6a-0e42-4079-8f5e-36ae69866f93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_sentence=\"This is a short sentence\"\n",
    "long_sentence=\"This sentence is considerable longer than the first one, while being still shorter than the model max length that is 512 words, it is also similar to the first sentence\"\n",
    "other_long_sentence=\"In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since. And then I met Gatsby!\"\n",
    "\n",
    "short_tokenized=tokenizer(short_sentence)\n",
    "long_tokenized=tokenizer(long_sentence)\n",
    "other_long_tokenized=tokenizer(other_long_sentence)\n",
    "\n",
    "short_tokenized\n",
    "short_tokenized[\"attention_mask\"]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The attention mask refers to which part of the text that the model is paying attention to. For large texts (texts that are larger than the models context window) you will find that either left or right side (depending on the setting of the model, which you can change - see tokenizer package documentation) will be set to 0. This means the model will ignore these words. Of the words that model is paying attention to, you will see that the words are converted to numbers, this is the job of the tokenizer. LLMs do not work on words, but numbers. So we need a way to convert words into numbers, this is the job of the tokenizer. For the most part this is just a dictionary lookup. This is especially true for smaller models with smaller vocabulary.",
   "id": "dd64519f69035177"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59d01199-b305-426c-a4a0-db324f888289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2023, 2003, 1037, 2460, 6251, 102]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_tokenized[\"input_ids\"] # the integers corresponding to each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1f8fe30-4de8-4184-9a63-3d0790310d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] this is a short sentence [SEP]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(short_tokenized[\"input_ids\"]) # the model is uncased so everything is always lower case, this means the words Alper and alper are the same, there are cased models where there is a difference, you can pick whiever suits your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cef14f84-f709-42d1-804e-489492cbe48f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 768]), torch.Size([34, 768]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_features=extractor(short_sentence)\n",
    "long_features=extractor(long_sentence)\n",
    "\n",
    "short_features[0].shape, long_features[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c2e954-f010-483c-9401-bee68116b426",
   "metadata": {},
   "source": [
    "We cannot compare tensors/matrices of different sizes, we need to somewhow need to convert them to the same shape, one way to do that is to take the mean across one dimension and end up with the same lenth vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "634e8dac-307b-44fb-b738-42ac5d0e5100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "short_mean=short_features[0].mean(axis=0)\n",
    "long_mean=long_features[0].mean(axis=0)\n",
    "\n",
    "from torch.nn import CosineSimilarity\n",
    "from torch.linalg import norm\n",
    "\n",
    "cos_sim=CosineSimilarity(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f817792-d522-4147-9477-74d20b944354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7598)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(short_mean, long_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cbbf69f-aca1-40a8-8d2b-255e82e82988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8372)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(torch.from_numpy(em1), torch.from_numpy(em2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccf8933c-9d93-4568-8d72-6b797b722697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1328)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(torch.from_numpy(em1), torch.from_numpy(em3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77cc289-bc43-490a-bd5d-cbb8b5953a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
