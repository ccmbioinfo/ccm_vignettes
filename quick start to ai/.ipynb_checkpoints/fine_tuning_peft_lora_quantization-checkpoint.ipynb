{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecd3975a-24b6-4098-bea3-fa064bea6d13",
   "metadata": {},
   "source": [
    "# CCM AI syncup session2\n",
    "\n",
    "This time we will be working on finetuning a model first and training a model from scracth second, in either case we will NOT be generating a new architecture but pick an exsiting one. \n",
    "\n",
    "## Fine tuning\n",
    "\n",
    "Fine tuning refers to taking a pre-trained model (This can be full training for a specific task or pre-trained as in masked language modelling) and adjusting it for a specific task ahead using a more limited dataset. \n",
    "Today we will use distilbert-base-uncased because it is little and we will use it for sentence classification, but there are many different tasks and huggingface documentation has decent examples of most of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2775320a-8219-414f-b043-11c20da5cfb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9083235d3d4d1f8d1648ba5dd45c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edcd5a3f2d4409c861cb159375133f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9face301ebbf48e69d252abad291a50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948aba7d4f7d4019b155b485a68d83b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190f336d736b463fb11640bd8b27c5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb906f8814bc4b05b2436272df6b8e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe3171c665545478bbd3f3cfadf51d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load an existing dataset, we will be doing sentiment analysis\n",
    "\n",
    "from datasets import load_dataset\n",
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398fce2a-4603-401d-9fc9-356d53f4c27e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichÃ©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e7d6fae-983a-4ac0-ab85-d99568a4aed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this is more of a convenience, as long as you remember which label is which you can do this after the fact at inference time, really doesnt matter\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fbe4541-11d7-4424-b15f-ff8240711826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\", padding=True, truncation=True, max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "874d3515-3f91-4e2e-b3e8-1273a8321f33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab39ebda-d699-437a-8f98-dc4890446559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f684833-4f64-43ba-a278-44f6dd1a9029",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5a547c30af4d3e9b45d43ddd943fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e08dd6c562417e8f777bc9bf33ed99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47e631d6f6f467c824acefd34f166ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a19afc74-fcac-418b-b097-824a04053dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'test', 'unsupervised']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(imdb.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "320f0792-f585-4cff-b597-ee358c4f30ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\") #there is also prec, recall, f1 etc.\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00923bbe-dd4b-404f-9c24-56baa6823fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5, #most important parameter when fine-tuning\n",
    "    per_device_train_batch_size=16, # as large as your gpu would allow\n",
    "    per_device_eval_batch_size=16, #same as above\n",
    "    num_train_epochs=2, # better to overshoot and load a previous checkpoint\n",
    "    weight_decay=0.01, # a small value (this is reasonable) to prevent overfitting, just as learning rate it is a trial and error, \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae356087-4b64-440b-bccf-8bbca9ff6c52",
   "metadata": {},
   "source": [
    "While these parameters are important the most imporatnt thing is your data and your labels, if they are not good there is nothing you can do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f16b06aa-f8e2-4a65-87c3-66ab6e87b2c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb[\"train\"], #need to pass tokenized dataset\n",
    "    eval_dataset=tokenized_imdb[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edba115-3530-4322-b3e8-f1a9c8a5a3b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20cec2a-2ea5-45c4-aad0-d0c0ec0e4ab8",
   "metadata": {},
   "source": [
    "## Training from scratch\n",
    "\n",
    "Unlike fine-tuning this will take a lot more resources because we are training the whole model weights not just the last layer, we will be training a masked language model for Esperanto again using the distilbert-base-uncased model architecture, we will need to generate our own tokenizer, along with a bunch of other functions for masked language modelling. \n",
    "\n",
    "### Tokenizer\n",
    "\n",
    "There are many different kinds of tokenizers, we will be using the simplest one word-piece tokenizer, you can use BytePairEncoding (BPE) or other lemma based ones like GPT uses. Check huggingface tokenizer documentation for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7b0477-6bc4-4e38-877f-2b9d5b2dbb37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "\n",
    "# each tokenizer have different parameters, you will need to check the documentation\n",
    "# in this case we are creating a tokenizer with 50K vocab, with 1000 different characters, since we are training a single\n",
    "# language model this probably is overkill but if you are training multiple languages, or you have symbols, emojis etc\n",
    "# you might want to keep the number large, also you will need to check your encoding, you cannot have emojis in ASCII for example\n",
    "tokenizer.train(files=\"epo_literature_2011_300K-sentences.txt\", \n",
    "                vocab_size=50_000, min_frequency=2,\n",
    "               limit_alphabet=1000,\n",
    "               special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693ebd95-ee51-4751-a186-35c89e006fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer/vocab.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809e8791-1890-4ac3-b088-83c084f71d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers.implementations import BertWordPieceTokenizer \n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    \"./tokenizer/vocab.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b8c6de-9629-4df8-8c45-663d023ad650",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'mi', 'estas', 'julie', '##n', '.', '[SEP]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Mi estas Julien.\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38cf9129-4aee-40be-939c-12aa1b5e1059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig #RobertaConfig\n",
    "\n",
    "config = DistilBertConfig(\n",
    "    vocab_size=50_000,\n",
    "    max_position_embeddings=514)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e42fdc6a-840d-4e08-a594-475266b47b92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"./tokenizer/\", max_len=512)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3724a5a2-f648-46fd-857d-b8b0c468072d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertForMaskedLM\n",
    "model = DistilBertForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "956292db-baf9-494e-b9ba-d2213fc4f154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e440e8c-a43f-416a-a726-7b3a1489121b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"epo_literature_2011_300K-sentences.txt\", header=None, sep=\"\\t\")\n",
    "data=data.rename(columns={0:\"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d47505ff-cb49-4fd9-b1b5-888169d20989",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10 La trajno forlasis Vincovci (malfrue). 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101 De Ã§iu malbona vojo mi detenas mian piedon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103 Kiel dolÃ§a estas por mia palato Via vorto!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103 MalfeliËa homo plendas pro tio ke neniu ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104 Verkisto diras pri kolego: âLi estas sprit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276699</th>\n",
       "      <td>Ãi havis larmojn en la okuloj, kiam Ã¾ia patro ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276700</th>\n",
       "      <td>Ãi povas eliri vespere, sen nerveksciteco dum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276701</th>\n",
       "      <td>Ãi respondis: - FranÃ¦jo, tion mi ne scias..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276702</th>\n",
       "      <td>Ãi ridetis kaj respondis: - Jes, sed Jesuo dez...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276703</th>\n",
       "      <td>Ãtelo 880. Kiu estas la unua el Ã¦iuj naturaj r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276704 rows Ã 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "0       0.10 La trajno forlasis Vincovci (malfrue). 0....\n",
       "1       101 De Ã§iu malbona vojo mi detenas mian piedon...\n",
       "2          103 Kiel dolÃ§a estas por mia palato Via vorto!\n",
       "3       103 MalfeliËa homo plendas pro tio ke neniu ha...\n",
       "4       104 Verkisto diras pri kolego: âLi estas sprit...\n",
       "...                                                   ...\n",
       "276699  Ãi havis larmojn en la okuloj, kiam Ã¾ia patro ...\n",
       "276700  Ãi povas eliri vespere, sen nerveksciteco dum ...\n",
       "276701        Ãi respondis: - FranÃ¦jo, tion mi ne scias..\n",
       "276702  Ãi ridetis kaj respondis: - Jes, sed Jesuo dez...\n",
       "276703  Ãtelo 880. Kiu estas la unua el Ã¦iuj naturaj r...\n",
       "\n",
       "[276704 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5073ce4-8ad0-48fe-88d4-94c19c737b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset=Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7db09ca3-a521-472e-b99d-9ba591a2cec3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f952712baf4265a2870509f113d905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/276704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "dataset_tokenized=dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44facdcf-a424-49bf-a794-a10f16ffabce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00bd7c29-e437-4782-9d04-5d95228b9443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5, #most important parameter when fine-tuning\n",
    "    per_device_train_batch_size=16, # as large as your gpu would allow\n",
    "    num_train_epochs=2, # better to overshoot and load a previous checkpoint\n",
    "    weight_decay=0.01, # a small value (this is reasonable) to prevent overfitting, just as learning rate it is a trial and error, \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_tokenized,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb2406-dc02-4728-8f95-4229c7090654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aaa16d-5c2b-4c3d-9d4a-6c784b8d8b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
