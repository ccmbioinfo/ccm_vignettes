{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecd3975a-24b6-4098-bea3-fa064bea6d13",
   "metadata": {},
   "source": [
    "# Introduction to AI part 2 fine tuning\n",
    "\n",
    "This time we will be working on finetuning a model first and training a model from scracth second, in either case we will NOT be generating a new architecture but pick an exsiting one. \n",
    "\n",
    "## Fine tuning\n",
    "\n",
    "Fine tuning refers to taking a pre-trained model (This can be full training for a specific task or pre-trained as in masked language modelling) and adjusting it for a specific task ahead using a more limited dataset. \n",
    "Today we will use distilbert-base-uncased because it is little and we will use it for sentence classification, but there are many different tasks and huggingface documentation has decent examples of most of them. "
   ]
  },
  {
   "cell_type": "code",
   "id": "2775320a-8219-414f-b043-11c20da5cfb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# load an existing dataset, we will be doing sentiment analysis\n",
    "\n",
    "from datasets import load_dataset\n",
    "imdb = load_dataset(\"imdb\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "398fce2a-4603-401d-9fc9-356d53f4c27e",
   "metadata": {
    "tags": []
   },
   "source": [
    "imdb[\"test\"][0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4e7d6fae-983a-4ac0-ab85-d99568a4aed8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#this is more of a convenience, as long as you remember which label is which you can do this after the fact at inference time, really doesnt matter\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the previous notebook we talked about Autotokenizers and Autmomodels (or at least used them). These are base classes that can access a model for supported architectures (and there are a lot!). The `.from_pretrained` methods are used to access existing models. These can be models from huggingface but can also be models that you have fine tuned and now want to use for inference.\n",
    "\n",
    "The TrainingArguments, and Trainer classes store the information that you need to start t"
   ],
   "id": "f6dd97e13f5833da"
  },
  {
   "cell_type": "code",
   "id": "1fbe4541-11d7-4424-b15f-ff8240711826",
   "metadata": {
    "tags": []
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\", padding=True, truncation=True, max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will create a preprocess function to set up our tokenizer with all the options that it supports. In this case we will remove all extra text via truncation, we will add a bunch of 0s to the short text until we reach the max lenght which is 512. While you can use change this arbitrarily on your tokenizer you cannot for your model and will not make a difference once you hit the max lenghts. If you want to change the context lenght of a model you will need to 1) change its architecture and 2) re-train it. That said there are many model with greatly varying context windows from 256 to 1 million.\n",
    "\n",
    "We will then use this preprocess function to tokenize our dataset and get it ready for inference. It's a small dataset and we can use the entire thing in memory but when it's larger than our RAM can hold you can use the [datasets](https://huggingface.co/docs/datasets/index) package to create data streams. That feature is not discussed here but it is very well documented."
   ],
   "id": "b7cc5ee24077c04c"
  },
  {
   "cell_type": "code",
   "id": "ab39ebda-d699-437a-8f98-dc4890446559",
   "metadata": {
    "tags": []
   },
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f684833-4f64-43ba-a278-44f6dd1a9029",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a19afc74-fcac-418b-b097-824a04053dc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "list(imdb.keys())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The [evaluate]() package holds several useful functions for well, evaluation. You can load your favourite one or you can pass an arbitrary function. We then need to wrap our metric in yet another function, the reason for this is not every model outputs things the same way, so you will need to change the outputs of your model and your inputs in a way that you can pass it to the metric function. In he example above we are taking the output of the model `eval_pred`. This is a piece of data that is in our test/validation set that we know the labels to. Then we pick the best (`np.argmax`) prediction. This is what the accuracy metric needs for its compute method.",
   "id": "94045cf7cd5445ee"
  },
  {
   "cell_type": "code",
   "id": "320f0792-f585-4cff-b597-ee358c4f30ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\") #there is also prec, recall, f1 etc.\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we define how we are going to train our model. There are many, many more options but the transformers package has already have a lot of very sensible defaults and they generally a very good start. As a general rule you will not go from 70% accuracy to 99% accuracy by playing around with hyperparameters, this will be more like from 85 to 89% on a good day. Like any ML model the data quality and the model's ability to comprehend all of its dimensions are the most important factors.",
   "id": "1801649e9e95d71d"
  },
  {
   "cell_type": "code",
   "id": "00923bbe-dd4b-404f-9c24-56baa6823fc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5, #most important parameter when fine-tuning\n",
    "    per_device_train_batch_size=16, # as large as your gpu would allow\n",
    "    per_device_eval_batch_size=16, #same as above\n",
    "    num_train_epochs=2, # better to overshoot and load a previous checkpoint\n",
    "    weight_decay=0.01, # a small value (this is reasonable) to prevent overfitting, just as learning rate it is a trial and error, \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae356087-4b64-440b-bccf-8bbca9ff6c52",
   "metadata": {},
   "source": [
    "While these parameters are important the most imporatnt thing is your data and your labels, if they are not good there is nothing you can do. "
   ]
  },
  {
   "cell_type": "code",
   "id": "f16b06aa-f8e2-4a65-87c3-66ab6e87b2c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb[\"train\"], #need to pass tokenized dataset\n",
    "    eval_dataset=tokenized_imdb[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5edba115-3530-4322-b3e8-f1a9c8a5a3b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f20cec2a-2ea5-45c4-aad0-d0c0ec0e4ab8",
   "metadata": {},
   "source": [
    "## Training from scratch\n",
    "\n",
    "Unlike fine-tuning this will take a lot more resources because we are training the whole model weights not just the last layer, we will be training a masked language model for Esperanto again using the distilbert-base-uncased model architecture, we will need to generate our own tokenizer, along with a bunch of other functions for masked language modelling. \n",
    "\n",
    "### Tokenizer\n",
    "\n",
    "There are many different kinds of tokenizers, we will be using the simplest one word-piece tokenizer, you can use BytePairEncoding (BPE) or other lemma based ones like GPT uses. Check huggingface tokenizer documentation for more details"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd7b0477-6bc4-4e38-877f-2b9d5b2dbb37",
   "metadata": {
    "tags": []
   },
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True)\n",
    "\n",
    "# each tokenizer have different parameters, you will need to check the documentation\n",
    "# in this case we are creating a tokenizer with 50K vocab, with 1000 different characters, since we are training a single\n",
    "# language model this probably is overkill but if you are training multiple languages, or you have symbols, emojis etc\n",
    "# you might want to keep the number large, also you will need to check your encoding, you cannot have emojis in ASCII for example\n",
    "tokenizer.train(files=\"epo_literature_2011_300K-sentences.txt\", \n",
    "                vocab_size=50_000, min_frequency=2,\n",
    "               limit_alphabet=1000,\n",
    "               special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "693ebd95-ee51-4751-a186-35c89e006fb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.save_model(\"tokenizer\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There are many way to tokenize a text, this can be as simple as splitting the text at every whitespace or as complicated as using a rule based lingusitics model (like spacy) to get a words root and any prefixes and suffixes and tokenize them separately. A good starting point is generally a [bytepair encoding tokenizer](https://huggingface.co/learn/llm-course/en/chapter6/5) .",
   "id": "94801ddd162c3258"
  },
  {
   "cell_type": "code",
   "id": "809e8791-1890-4ac3-b088-83c084f71d27",
   "metadata": {
    "tags": []
   },
   "source": [
    "from tokenizers.implementations import BertWordPieceTokenizer \n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    \"./tokenizer/vocab.txt\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65b8c6de-9629-4df8-8c45-663d023ad650",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer.encode(\"Mi estas Julien.\").tokens"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have our tokenizer we need to pick a model. Usually you pick the model and the tokenizer that comes with it. Except for rare cases you cannot really use a different tokenizer than the one model comes with. When we are starting a model from scratch we do not need the previously trained weights. All we need is the model configuration that you can download or add your own. In the hugginface repository you can see this configuration under the models files. [Here](https://huggingface.co/distilbert/distilbert-base-uncased/blob/main/config.json) is an example",
   "id": "1fb09a2210241841"
  },
  {
   "cell_type": "code",
   "id": "38cf9129-4aee-40be-939c-12aa1b5e1059",
   "metadata": {
    "tags": []
   },
   "source": [
    "from transformers import DistilBertConfig #RobertaConfig\n",
    "\n",
    "config = DistilBertConfig(\n",
    "    vocab_size=50_000,\n",
    "    max_position_embeddings=514)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e42fdc6a-840d-4e08-a594-475266b47b92",
   "metadata": {
    "tags": []
   },
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"./tokenizer/\", max_len=512)\n",
    "     "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3724a5a2-f648-46fd-857d-b8b0c468072d",
   "metadata": {
    "tags": []
   },
   "source": [
    "from transformers import DistilBertForMaskedLM\n",
    "model = DistilBertForMaskedLM(config=config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have everything we need, except for the most important part, the data. I will not be sharing this dataset because it's too big to put in github but it's basically a bunch of sentences written in esperanto. It does not have any labels. What we are doing at the moment is pre-training. We are just teaching the model to speak esperanto. Once that is done we can work on fine tuning (like above) for a specific task. Bert models are \"masked language models\", which means for a given data point we hide n% (usually 15-15) of it with a `[MASK]` token as ask the model to predict what goes in there. As the model goes through examples it will start learning the relationships between different tokens.\n",
    "\n",
    "During fine tuning we will expoit this learned relationship between tokens and ask the model to assign labels to your liking."
   ],
   "id": "896c43967fa6eb01"
  },
  {
   "cell_type": "code",
   "id": "956292db-baf9-494e-b9ba-d2213fc4f154",
   "metadata": {
    "tags": []
   },
   "source": [
    "from datasets import Dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e440e8c-a43f-416a-a726-7b3a1489121b",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"epo_literature_2011_300K-sentences.txt\", header=None, sep=\"\\t\")\n",
    "data=data.rename(columns={0:\"text\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d47505ff-cb49-4fd9-b1b5-888169d20989",
   "metadata": {
    "tags": []
   },
   "source": [
    "data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5073ce4-8ad0-48fe-88d4-94c19c737b76",
   "metadata": {
    "tags": []
   },
   "source": [
    "from datasets import Dataset\n",
    "dataset=Dataset.from_pandas(data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7db09ca3-a521-472e-b99d-9ba591a2cec3",
   "metadata": {
    "tags": []
   },
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "dataset_tokenized=dataset.map(preprocess_function)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The data collator class is the part that manages to set up this masking, all we need to specify what kind of model we are dealing with and some additional parameters. Of course we need our tokenizer because without it we do not have the `[MASK]` token.",
   "id": "cc9b9c1981b9589e"
  },
  {
   "cell_type": "code",
   "id": "44facdcf-a424-49bf-a794-a10f16ffabce",
   "metadata": {
    "tags": []
   },
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "00bd7c29-e437-4782-9d04-5d95228b9443",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5, #most important parameter when fine-tuning\n",
    "    per_device_train_batch_size=16, # as large as your gpu would allow\n",
    "    num_train_epochs=2, # better to overshoot and load a previous checkpoint\n",
    "    weight_decay=0.01, # a small value (this is reasonable) to prevent overfitting, just as learning rate it is a trial and error, \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_tokenized,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ffb2406-dc02-4728-8f95-4229c7090654",
   "metadata": {
    "tags": []
   },
   "source": [
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "09aaa16d-5c2b-4c3d-9d4a-6c784b8d8b5d",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
